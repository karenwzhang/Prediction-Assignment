---
title: "Prediction Assignment"
author: "Karen"
date: "2023-07-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

In this project, we use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise, correspoding to the variable "class" in the data set. We train 3 models: decesion tree, random forest, and gradient boost tree using k-folds cross validation on the training data set. Based on the accuracy and out-of-sample error, we select the best model and use it to predict the test data set.

## Download libraries and data

Download the relevant libraries:

```{r, results='hide'}
library(ggplot2)
library(caret)
library(kernlab)
library(rattle)
set.seed(1234)
```
Download data set:
```{r}
train<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
test<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
dim(train)
dim(test)
```

## Cleaning the data
Remove near zero variance variables:
```{r}
nvz<-nearZeroVar(train)
train<-train[,-nvz]
dim(train)
```
 Remove na columns:
```{r}
train<-train[,colMeans(is.na(train))<0.9]
```
Remove irrelevant data:
```{r}
train<-train[,-c(1:7)]
```

We split the training data set into a training and validation data set.
```{r}
intrain<-createDataPartition(y=train$classe, p=0.7, list=FALSE)
train<-train[intrain,]
validation<-train[-intrain,]
```

## Create and test models
We test 3 popular models, indluding decision tree, random forest, and gradient boosted trees.

Set up 3-folds control for training set.
```{r}
control<-trainControl(method="cv", number=3, verboseIter =F)
```

### 1. Decision tree
```{r}
DT<-train(classe~., data=train, method="rpart", trControl=control)
pred_DT<-predict(DT, validation)
cm_DT<-confusionMatrix(pred_DT, factor(validation$classe))
cm_DT
```
The accuracy of decision tree model is 0.4853, and its out of sample error is 0.5147.

### 2. Random Forest
```{r}
RF<-train(classe~., data=train, method="rf", trControl=control, tuneLength = 5)
pred_RF<-predict(RF, validation)
cm_RF<-confusionMatrix(pred_RF, factor(validation$classe))
cm_RF
```
The accuracy of random forest is almost equal to 1.

### 3. Gradiant boosted forest
```{r}
GBM<-train(classe~., data=train, method="gbm", trControl=control, tuneLength = 5, verbose=F)
pred_GBM<-predict(GBM, validation)
cm_GBM<-confusionMatrix(pred_GBM, factor(validation$classe))
cm_GBM
```
The accuracy of gradient boosted tree model is 0.999, and out of sample error is 0.0001.
So the best result is random forest model, with accuracy and out of sample error.

### Prediction on test set
Run random forest model on the test set.
```{r}
pred<-predict(RF, test)
print(pred)
```
